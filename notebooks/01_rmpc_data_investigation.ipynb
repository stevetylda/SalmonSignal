{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f739d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------- #\n",
    "#                              MODULES                              #\n",
    "\n",
    "# Standard Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-Party Modules\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import plotly.express as px\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from typing import Literal\n",
    "import h3\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import dcor\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#                                                                   #\n",
    "# ----------------------------------------------------------------- #\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "#                             FUNCTIONS                             #\n",
    "\n",
    "##############\n",
    "# COLLECTION\n",
    "\n",
    "\n",
    "# RPMC - WebPage Download\n",
    "def download_rmpc_data(\n",
    "    base_url=\"https://www.rmpc.org/pub/data\",\n",
    "    output_dir=\"../data/raw/RMPC/LOOKUPS\",\n",
    "    filename=\"LC042_ALL_FULLSET.csv\",\n",
    "    overwrite=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads a lookup CSV file from RMPC if not already downloaded.\n",
    "\n",
    "    Parameters:\n",
    "        base_url (str): Base URL of the RMPC file host.\n",
    "        output_dir (str): Local directory to save the downloaded file.\n",
    "        filename (str): Name of the file to download.\n",
    "        overwrite (bool): If True, force re-download even if file exists.\n",
    "\n",
    "    Returns:\n",
    "        str or None: Path to the downloaded file, or None if download failed.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/{filename}\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(output_path) and not overwrite:\n",
    "        print(f\"‚ö†Ô∏è  File already exists, skipping download: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    try:\n",
    "        print(f\"‚¨áÔ∏è  Downloading {filename} from RMPC...\")\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(f\"‚úÖ File downloaded successfully: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"‚ùå HTTP error: {e}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Something went wrong: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Download WDFW Parquet\n",
    "def update_wdfw_parquet(\n",
    "    output_dir=\"../data/raw/RMPC/WDFW\",\n",
    "    base_url=\"https://www.rmpc.org/pub/data/\",\n",
    "    pattern=\"CS042_WDFW_.*\\\\.csv\",\n",
    "):\n",
    "    raw_dir = os.path.join(output_dir, \"raw\", \"RMPC\", \"WDFW\")\n",
    "    parquet_path = os.path.join(\n",
    "        output_dir, \"processed\", f\"{pattern[0:2]}042_WDFW_FULL.parquet\"\n",
    "    )\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(parquet_path), exist_ok=True)\n",
    "\n",
    "    # Load existing parquet to see what's already included\n",
    "    existing_files = set()\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"üì¶ Loading existing parquet file: {parquet_path}\")\n",
    "        existing_df = pd.read_parquet(parquet_path)\n",
    "        if \"source_filename\" in existing_df.columns:\n",
    "            existing_files = set(existing_df[\"source_filename\"].unique())\n",
    "        else:\n",
    "            raise ValueError(\"Missing 'source_filename' column in existing parquet!\")\n",
    "\n",
    "    # Scrape available files from website\n",
    "    # print(f\"üåê Scraping file list from {base_url}\")\n",
    "    response = requests.get(base_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "    csv_files = [f for f in links if re.match(pattern, f)]\n",
    "    new_files = [f for f in csv_files if f not in existing_files]\n",
    "\n",
    "    print(f\"üîé Found {len(csv_files)} total files, {len(new_files)} new to download.\")\n",
    "\n",
    "    # Download and read new CSVs\n",
    "    new_data = []\n",
    "    for fname in new_files:\n",
    "        file_url = f\"{base_url.rstrip('/')}/{fname}\"\n",
    "        local_path = os.path.join(raw_dir, fname)\n",
    "        # print(f\"‚¨áÔ∏è Downloading: {fname}\")\n",
    "        try:\n",
    "            r = requests.get(file_url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "            df = pd.read_csv(local_path, low_memory=False)\n",
    "            df[\"source_filename\"] = fname\n",
    "            new_data.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading {fname}: {e}\")\n",
    "\n",
    "    if not new_data:\n",
    "        print(\"üì≠ No new files to process. Parquet is up to date.\")\n",
    "        return parquet_path\n",
    "\n",
    "    combined_new = pd.concat(new_data, ignore_index=True)\n",
    "\n",
    "    if os.path.exists(parquet_path):\n",
    "        # print(\"üß¨ Appending to existing Parquet...\")\n",
    "        full_df = pd.concat([existing_df, combined_new], ignore_index=True)\n",
    "    else:\n",
    "        print(\"üìÅ Creating new Parquet...\")\n",
    "        full_df = combined_new\n",
    "\n",
    "    full_df = full_df.astype(str)\n",
    "    full_df.to_parquet(parquet_path, engine=\"fastparquet\", index=False)\n",
    "    print(f\"‚úÖ Saved combined data to: {parquet_path}\")\n",
    "\n",
    "    return parquet_path\n",
    "\n",
    "\n",
    "##############\n",
    "# PROCESSING\n",
    "\n",
    "\n",
    "## Compute Sunday from Statistical Week\n",
    "def compute_sunday(row):\n",
    "    jan1 = datetime(int(row[\"catch_year\"]), 1, 1)\n",
    "    first_monday = jan1 + timedelta(days=(7 - jan1.weekday()) % 7)\n",
    "    return first_monday + timedelta(weeks=int(row[\"stat_week\"]) - 1, days=6)\n",
    "\n",
    "\n",
    "## Catch Data Preprocessing\n",
    "def preprocess_rmpc_catch_data(catch_data):\n",
    "    cs = catch_data.copy()\n",
    "\n",
    "    cs.columns = cs.columns.str.strip().str.lower()\n",
    "\n",
    "    # Adjust Catch Code\n",
    "    cs[\"catch_location_code\"] = cs[\"catch_location_code\"].str.replace(\"  \", \" \")\n",
    "\n",
    "    # Add State Code\n",
    "    cs[\"state_code\"] = cs[\"catch_location_code\"].str[0]\n",
    "\n",
    "    # Add Water Type\n",
    "    cs[\"water_type_code\"] = cs[\"catch_location_code\"].str[1]\n",
    "\n",
    "    # Add Sector\n",
    "    cs[\"sector_code\"] = cs[\"catch_location_code\"].str[2]\n",
    "\n",
    "    # Add Region\n",
    "    cs[\"region_code\"] = cs[\"catch_location_code\"].str[3:5]\n",
    "\n",
    "    # Add Statistical Area\n",
    "    cs[\"statistical_area\"] = cs[\"catch_location_code\"].str[5:7]\n",
    "\n",
    "    # Filter to Statistical Week Period\n",
    "    cs = cs[cs.period_type == \"6\"]\n",
    "    cs[\"stat_week\"] = cs[\"period\"]\n",
    "\n",
    "    # Build Date\n",
    "    cs[\"stat_week_sunday\"] = cs.apply(compute_sunday, axis=1)\n",
    "\n",
    "    # Add Number Caught\n",
    "    cs[\"number_caught\"] = cs[\"number_caught\"].astype(float)\n",
    "\n",
    "    return cs\n",
    "\n",
    "\n",
    "## Recovery Data Preprocessing\n",
    "def preprocess_rmpc_recovery_data(recovery_data):\n",
    "    rs = recovery_data.copy()\n",
    "\n",
    "    rs.columns = rs.columns.str.strip().str.lower()\n",
    "\n",
    "    # Adjust Catch Code\n",
    "    rs[\"recovery_location_code\"] = rs[\"recovery_location_code\"].str.replace(\"  \", \" \")\n",
    "\n",
    "    # Add State Code\n",
    "    rs[\"state_code\"] = rs[\"recovery_location_code\"].str[0]\n",
    "\n",
    "    # Add Water Type\n",
    "    rs[\"water_type_code\"] = rs[\"recovery_location_code\"].str[1]\n",
    "\n",
    "    # Add Sector\n",
    "    rs[\"sector_code\"] = rs[\"recovery_location_code\"].str[2]\n",
    "\n",
    "    # Add Region\n",
    "    rs[\"region_code\"] = rs[\"recovery_location_code\"].str[3:5]\n",
    "\n",
    "    # Add Statistical Area\n",
    "    rs[\"statistical_area\"] = rs[\"recovery_location_code\"].str[5:7]\n",
    "\n",
    "    # Filter to Statistical Week Period\n",
    "    rs = rs[rs.period_type == \"6\"]\n",
    "    rs[\"stat_week\"] = rs[\"period\"]\n",
    "\n",
    "    # Build Date\n",
    "    rs = rs.rename(columns={\"run_year\": \"catch_year\"})\n",
    "    rs[\"stat_week_sunday\"] = rs.apply(compute_sunday, axis=1)\n",
    "\n",
    "    # Add Number Caught\n",
    "    rs[\"number_caught\"] = 1  # rs[\"number_caught\"].astype(float)\n",
    "\n",
    "    return rs\n",
    "\n",
    "\n",
    "## Extract General Code\n",
    "def extract_general_code(code):\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    s = str(code).strip().upper()  # normalize case & whitespace\n",
    "    # Match: digits + optional - + digits at start, ignore trailing letters\n",
    "    m = re.match(r\"^(\\d+(-\\d+)?)\", s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "# Normalize Code\n",
    "def normalize_code(code):\n",
    "    return \"\".join(str(code).upper().split())\n",
    "\n",
    "\n",
    "# Extract Statistical Area\n",
    "def extract_stat_area(code):\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    code = str(code).upper().replace(\" \", \"\")\n",
    "\n",
    "    if code.startswith(\"3F\"):\n",
    "        return None  # skip for now\n",
    "\n",
    "    # Match 3M + 5 digits + optional letter suffix\n",
    "    match = re.match(r\"3M(\\d{4})(\\d)([A-Z]?)\", code)\n",
    "    if match:\n",
    "        # last digit + optional letter suffix\n",
    "        last_digit = match.group(3) if match.group(3) else \"\"\n",
    "        stat_area_num = match.group(2)  # the digit before last digit\n",
    "        stat_area = match.group(2) + match.group(3)\n",
    "        # Actually we want last two digits + optional letter\n",
    "        stat_area = match.group(2) + match.group(3)  # hmm, re-check this logic\n",
    "        # better: last two digits + optional letter suffix\n",
    "        stat_area = code[-2:]\n",
    "        # Wait, need to be precise\n",
    "\n",
    "        # Let's re-match to last two digits + optional letter:\n",
    "        match2 = re.search(r\"(\\d{2})([A-Z]?)$\", code)\n",
    "        if match2:\n",
    "            return match2.group(1) + match2.group(2)\n",
    "\n",
    "    # fallback: number + letter suffix anywhere at end\n",
    "    match_simple = re.search(r\"(\\d{1,2})([A-Z]?)$\", code)\n",
    "    if match_simple:\n",
    "        return match_simple.group(1).zfill(2) + match_simple.group(2)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Normalize Statistical Area\n",
    "def normalize_stat_area(code):\n",
    "    if not code or pd.isna(code):\n",
    "        return None\n",
    "    m = re.match(r\"0*(\\d+)([A-Z]?)\", code.upper().replace(\" \", \"\"))\n",
    "    if m:\n",
    "        num = m.group(1)\n",
    "        letter = m.group(2) or \"\"\n",
    "        return num + letter\n",
    "    return None\n",
    "\n",
    "\n",
    "# Extract stat_area_name from catch_location_code\n",
    "def extract_stat_area(code):\n",
    "    if pd.isna(code):\n",
    "        return None\n",
    "    code = str(code).upper().replace(\" \", \"\")\n",
    "\n",
    "    if code.startswith(\"3F\"):\n",
    "        return None  # skip 3F for now\n",
    "\n",
    "    # Grab last two digits plus optional letter suffix from the end\n",
    "    match = re.search(r\"(\\d{2})([A-Z]?)$\", code)\n",
    "    if match:\n",
    "        return match.group(1) + match.group(2)\n",
    "\n",
    "    # Fallback simple extraction\n",
    "    match_simple = re.search(r\"(\\d{1,2})([A-Z]?)$\", code)\n",
    "    if match_simple:\n",
    "        return match_simple.group(1).zfill(2) + match_simple.group(2)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Main Processing Pipeline - Catch\n",
    "def main_preprocessing_catch(\n",
    "    cs, lookup_state_code, lookup_water_type, lookup_sector_type, psc_stat_area_lookup\n",
    "):\n",
    "    tmp = cs[\n",
    "        [\n",
    "            \"catch_location_code\",\n",
    "            \"state_code\",\n",
    "            \"water_type_code\",\n",
    "            \"sector_code\",\n",
    "            \"region_code\",\n",
    "            \"statistical_area\",\n",
    "        ]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    # Normalize and uppercase\n",
    "    tmp[\"cache_short_code\"] = tmp[\"catch_location_code\"].str[0:7]\n",
    "    for col in tmp.columns:\n",
    "        tmp[col] = tmp[col].astype(str).str.upper().str.strip()\n",
    "\n",
    "    # Merge in lookup tables\n",
    "    tmp = pd.merge(tmp, lookup_state_code, how=\"left\", on=\"state_code\")\n",
    "    tmp[\"state\"] = tmp[\"state\"].fillna(\"Other\")\n",
    "\n",
    "    tmp = pd.merge(tmp, lookup_water_type, how=\"left\", on=\"water_type_code\")\n",
    "    tmp[\"water_type\"] = tmp[\"water_type\"].fillna(\"Other\")\n",
    "\n",
    "    tmp = pd.merge(tmp, lookup_sector_type, how=\"left\", on=\"sector_code\")\n",
    "    tmp[\"sector\"] = tmp[\"sector\"].fillna(\"Other\")\n",
    "\n",
    "    # Extract normalized stat area name\n",
    "    tmp[\"stat_area_name\"] = tmp[\"catch_location_code\"].apply(extract_stat_area)\n",
    "\n",
    "    # Final normalization for short code\n",
    "    tmp[\"norm_code\"] = tmp[\"catch_location_code\"].apply(normalize_code)\n",
    "\n",
    "    # Prepare lookup dataframe\n",
    "    lookup_df = pd.DataFrame.from_dict(\n",
    "        psc_stat_area_lookup, orient=\"index\"\n",
    "    ).reset_index()\n",
    "    lookup_df = lookup_df.rename(columns={\"index\": \"stat_area_name\"})\n",
    "\n",
    "    # Normalize lookup keys\n",
    "    lookup_df[\"stat_area_name\"] = lookup_df[\"stat_area_name\"].apply(normalize_stat_area)\n",
    "\n",
    "    # Assume tmp is your dataframe with catch_location_code column\n",
    "    tmp[\"stat_area_name\"] = tmp[\"catch_location_code\"].apply(extract_stat_area)\n",
    "\n",
    "    # Normalize tmp stat_area_name to match lookup keys\n",
    "    tmp[\"stat_area_name\"] = tmp[\"stat_area_name\"].apply(normalize_stat_area)\n",
    "\n",
    "    # Merge marine area info\n",
    "    tmp = tmp.merge(lookup_df, how=\"left\", on=\"stat_area_name\")\n",
    "\n",
    "    # General Marine Area\n",
    "    tmp[\"general_marine_area\"] = tmp[\"marine_area\"].astype(str).str.split(\" - \").str[0]\n",
    "\n",
    "    # General Area 8 Adjustment\n",
    "    tmp[\"stat_area_name\"] = tmp[\"stat_area_name\"].str.replace(\"8A\", \"8-1\")\n",
    "    tmp[\"stat_area_name\"] = tmp[\"stat_area_name\"].str.replace(\"8D\", \"8-2\")\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "# Main processing pipeline\n",
    "def main_preprocessing_recover(\n",
    "    rc, lookup_state_code, lookup_water_type, lookup_sector_type, psc_stat_area_lookup\n",
    "):\n",
    "    tmp = rc[\n",
    "        [\n",
    "            \"recovery_location_code\",\n",
    "            \"state_code\",\n",
    "            \"water_type_code\",\n",
    "            \"sector_code\",\n",
    "            \"region_code\",\n",
    "            \"statistical_area\",\n",
    "        ]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    # Normalize and uppercase\n",
    "    tmp[\"recovery_short_code\"] = tmp[\"recovery_location_code\"].str[0:7]\n",
    "    for col in tmp.columns:\n",
    "        tmp[col] = tmp[col].astype(str).str.upper().str.strip()\n",
    "\n",
    "    # Merge in lookup tables\n",
    "    tmp = pd.merge(tmp, lookup_state_code, how=\"left\", on=\"state_code\")\n",
    "    tmp[\"state\"] = tmp[\"state\"].fillna(\"Other\")\n",
    "\n",
    "    tmp = pd.merge(tmp, lookup_water_type, how=\"left\", on=\"water_type_code\")\n",
    "    tmp[\"water_type\"] = tmp[\"water_type\"].fillna(\"Other\")\n",
    "\n",
    "    tmp = pd.merge(tmp, lookup_sector_type, how=\"left\", on=\"sector_code\")\n",
    "    tmp[\"sector\"] = tmp[\"sector\"].fillna(\"Other\")\n",
    "\n",
    "    # Extract normalized stat area name\n",
    "    tmp[\"stat_area_name\"] = tmp[\"recovery_location_code\"].apply(extract_stat_area)\n",
    "\n",
    "    # Final normalization for short code\n",
    "    tmp[\"norm_code\"] = tmp[\"recovery_location_code\"].apply(normalize_code)\n",
    "\n",
    "    # Prepare lookup dataframe\n",
    "    lookup_df = pd.DataFrame.from_dict(\n",
    "        psc_stat_area_lookup, orient=\"index\"\n",
    "    ).reset_index()\n",
    "    lookup_df = lookup_df.rename(columns={\"index\": \"stat_area_name\"})\n",
    "\n",
    "    # Normalize lookup keys\n",
    "    lookup_df[\"stat_area_name\"] = lookup_df[\"stat_area_name\"].apply(normalize_stat_area)\n",
    "\n",
    "    # Assume tmp is your dataframe with catch_location_code column\n",
    "    tmp[\"stat_area_name\"] = tmp[\"recovery_location_code\"].apply(extract_stat_area)\n",
    "\n",
    "    # Normalize tmp stat_area_name to match lookup keys\n",
    "    tmp[\"stat_area_name\"] = tmp[\"stat_area_name\"].apply(normalize_stat_area)\n",
    "\n",
    "    # Merge marine area info\n",
    "    tmp = tmp.merge(lookup_df, how=\"left\", on=\"stat_area_name\")\n",
    "\n",
    "    # General Marine Area\n",
    "    tmp[\"general_marine_area\"] = tmp[\"marine_area\"].astype(str).str.split(\" - \").str[0]\n",
    "\n",
    "    # General Area 8 Adjustment\n",
    "    tmp[\"stat_area_name\"] = tmp[\"stat_area_name\"].str.replace(\"8A\", \"8-1\")\n",
    "    tmp[\"stat_area_name\"] = tmp[\"stat_area_name\"].str.replace(\"8D\", \"8-2\")\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "# Get Filled Time Series\n",
    "def get_time_series_filled_out(cs_mapped_sums):\n",
    "    # Step 1: Get complete list of weeks from min to max\n",
    "    full_weeks = pd.date_range(\n",
    "        start=cs_mapped_sums[\"stat_week_sunday\"].min(),\n",
    "        end=cs_mapped_sums[\"stat_week_sunday\"].max(),\n",
    "        freq=\"W-SUN\",\n",
    "    )\n",
    "\n",
    "    # Step 2: Get all marine areas and species\n",
    "    areas = cs_mapped_sums[\"MARINE_AREA_LARGE\"].unique()\n",
    "    species = cs_mapped_sums[\"species\"].unique()\n",
    "\n",
    "    # Step 3: Cartesian product of all combinations\n",
    "    full_index = pd.MultiIndex.from_product(\n",
    "        [areas, full_weeks, species],\n",
    "        names=[\"MARINE_AREA_LARGE\", \"stat_week_sunday\", \"species\"],\n",
    "    )\n",
    "    full_df = full_index.to_frame(index=False)\n",
    "\n",
    "    # Step 4: Merge with actual data\n",
    "    merged = pd.merge(\n",
    "        full_df,\n",
    "        cs_mapped_sums[\n",
    "            [\"MARINE_AREA_LARGE\", \"stat_week_sunday\", \"species\", \"number_caught\"]\n",
    "        ],\n",
    "        on=[\"MARINE_AREA_LARGE\", \"stat_week_sunday\", \"species\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Step 5: Fill missing catches with 0\n",
    "    merged[\"number_caught\"] = merged[\"number_caught\"].fillna(0)\n",
    "\n",
    "    # Step 6: Reattach optional fields like species_name + geometry\n",
    "    merged = pd.merge(\n",
    "        merged,\n",
    "        cs_mapped_sums[[\"species\", \"species_name\"]].drop_duplicates(),\n",
    "        on=\"species\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    merged = pd.merge(\n",
    "        merged,\n",
    "        cs_mapped_sums[[\"MARINE_AREA_LARGE\", \"geometry\"]].drop_duplicates(),\n",
    "        on=\"MARINE_AREA_LARGE\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Step 7 (optional): Convert back to GeoDataFrame\n",
    "    gdf_filled = gpd.GeoDataFrame(\n",
    "        merged, geometry=\"geometry\", crs=wdfw_marine_areas.crs\n",
    "    )\n",
    "\n",
    "    return gdf_filled\n",
    "\n",
    "\n",
    "# Loan and Process Sightings\n",
    "def load_and_process_sighting_data(\n",
    "    directory: str,\n",
    "    date_col: str,\n",
    "    lat_col: str,\n",
    "    lon_col: str,\n",
    "    id_col: str,\n",
    "    h3_resolution: int,\n",
    "    start_date: str = None,\n",
    "    source: Literal[\"TMW\", \"ACARTIA\"] = \"TMW\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and process sighting data for TMW or Acartia.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to CSV files.\n",
    "        date_col (str): Column name containing datetime string.\n",
    "        lat_col (str): Latitude column name.\n",
    "        lon_col (str): Longitude column name.\n",
    "        id_col (str): Unique identifier or countable column.\n",
    "        h3_resolution (int): H3 resolution to use.\n",
    "        start_date (str, optional): Filter records to start at this date.\n",
    "        source (str): \"TMW\" or \"ACARTIA\", for minor formatting differences.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated sightings data with full date-grid coverage.\n",
    "    \"\"\"\n",
    "    print(directory)\n",
    "    # Read & concat all CSVs\n",
    "    if \".csv\" in directory:\n",
    "        data = pd.read_csv(directory)\n",
    "    else:\n",
    "        data = pd.concat(\n",
    "            [pd.read_csv(path) for path in glob.glob(f\"{directory}/*.csv\")]\n",
    "        )\n",
    "    data.columns = data.columns.str.upper()\n",
    "\n",
    "    # Parse date and geo\n",
    "    data[\"DATE\"] = data[date_col].str[:10]\n",
    "    data[\"LATITUDE\"] = pd.to_numeric(data[lat_col], errors=\"coerce\")\n",
    "    data[\"LONGITUDE\"] = pd.to_numeric(data[lon_col], errors=\"coerce\")\n",
    "    data = data.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n",
    "\n",
    "    # Calculate H3 grid\n",
    "    h3_col = f\"H3_GRID_{h3_resolution}\"\n",
    "    data[h3_col] = data.apply(\n",
    "        lambda x: h3.latlng_to_cell(x[\"LATITUDE\"], x[\"LONGITUDE\"], h3_resolution),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    data[\"DATE\"] = pd.to_datetime(data[\"DATE\"])\n",
    "    if start_date:\n",
    "        data = data[data[\"DATE\"] >= pd.to_datetime(start_date)]\n",
    "\n",
    "    # Aggregate sightings\n",
    "    data_agg = data.groupby(\n",
    "        [\"DATE\", \"LATITUDE\", \"LONGITUDE\", h3_col], as_index=False\n",
    "    ).agg(SIGHTING_COUNT=(id_col, \"count\"))\n",
    "\n",
    "    return data_agg\n",
    "\n",
    "\n",
    "## ANALYSIS\n",
    "def preprocess_and_plot(\n",
    "    df,\n",
    "    orca_col=\"SIGHTINGS_RATIO\",\n",
    "    salmon_col=\"STANDARIZED_CAUGHT\",\n",
    "    window_size=4,\n",
    "    max_lag=16,\n",
    "):\n",
    "    # 1. Smooth with rolling mean (centered)\n",
    "    df[f\"{orca_col}_SMOOTH\"] = (\n",
    "        df[orca_col].rolling(window=window_size, center=True).mean()\n",
    "    )\n",
    "    df[f\"{salmon_col}_SMOOTH\"] = (\n",
    "        df[salmon_col].rolling(window=window_size, center=True).mean()\n",
    "    )\n",
    "\n",
    "    # Drop NaNs introduced by smoothing\n",
    "    df = df.dropna(subset=[f\"{orca_col}_SMOOTH\", f\"{salmon_col}_SMOOTH\"]).copy()\n",
    "\n",
    "    # 2. Standardize both\n",
    "    scaler = StandardScaler()\n",
    "    df[[f\"{orca_col}_SCALED\", f\"{salmon_col}_SCALED\"]] = scaler.fit_transform(\n",
    "        df[[f\"{orca_col}_SMOOTH\", f\"{salmon_col}_SMOOTH\"]]\n",
    "    )\n",
    "\n",
    "    # 3. STL Detrending (seasonal period 52 weeks)\n",
    "    stl_orca = STL(df[f\"{orca_col}_SCALED\"], period=52).fit()\n",
    "    stl_salmon = STL(df[f\"{salmon_col}_SCALED\"], period=52).fit()\n",
    "    df[f\"{orca_col}_DETREND\"] = df[f\"{orca_col}_SCALED\"] - stl_orca.trend\n",
    "    df[f\"{salmon_col}_DETREND\"] = df[f\"{salmon_col}_SCALED\"] - stl_salmon.trend\n",
    "\n",
    "    # 4. Remove outliers using z-score cutoff (e.g. abs(z) > 3)\n",
    "    for col in [f\"{orca_col}_DETREND\", f\"{salmon_col}_DETREND\"]:\n",
    "        z_scores = (df[col] - df[col].mean()) / df[col].std()\n",
    "        df = df[z_scores.abs() <= 3]\n",
    "\n",
    "    # 5. Define lagged correlation calculation function\n",
    "    def lagged_correlation(df, max_lag, orca_col, salmon_col):\n",
    "        results = []\n",
    "        for lag in range(-max_lag, max_lag + 1):\n",
    "            shifted_orca = df[orca_col].shift(lag)\n",
    "            valid = pd.concat([df[salmon_col], shifted_orca], axis=1).dropna()\n",
    "            if len(valid) > 0:\n",
    "                pearson_r, _ = pearsonr(valid[salmon_col], valid[orca_col])\n",
    "                mi = mutual_info_regression(\n",
    "                    valid[salmon_col].values.reshape(-1, 1),\n",
    "                    valid[orca_col].values,\n",
    "                    random_state=42,\n",
    "                )[0]\n",
    "                d_corr = dcor.distance_correlation(\n",
    "                    valid[salmon_col].values, valid[orca_col].values\n",
    "                )\n",
    "                results.append((lag, pearson_r, mi, d_corr))\n",
    "            else:\n",
    "                results.append((lag, float(\"nan\"), float(\"nan\"), float(\"nan\")))\n",
    "        return pd.DataFrame(\n",
    "            results, columns=[\"lag_weeks\", \"pearson_r\", \"mutual_info\", \"distance_corr\"]\n",
    "        )\n",
    "\n",
    "    # 6. Run lagged correlations on detrended data\n",
    "    lagged_df = lagged_correlation(\n",
    "        df, max_lag, f\"{orca_col}_DETREND\", f\"{salmon_col}_DETREND\"\n",
    "    )\n",
    "\n",
    "    # 7. Plot results\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=lagged_df[\"lag_weeks\"],\n",
    "            y=lagged_df[\"pearson_r\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Pearson r\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=lagged_df[\"lag_weeks\"],\n",
    "            y=lagged_df[\"mutual_info\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Mutual Info\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=lagged_df[\"lag_weeks\"],\n",
    "            y=lagged_df[\"distance_corr\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Distance Corr\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Lagged Correlation Metrics: Orca Sightings vs Chinook Catch (Preprocessed)\",\n",
    "        xaxis_title=\"Lag (weeks)\",\n",
    "        yaxis_title=\"Correlation / Mutual Info\",\n",
    "        legend_title=\"Metric\",\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "    return lagged_df\n",
    "\n",
    "\n",
    "def run_full_analysis(\n",
    "    orca_df,\n",
    "    salmon_df,\n",
    "    orca_col=\"SIGHTINGS_RATIO\",\n",
    "    salmon_col=\"STANDARIZED_CAUGHT\",\n",
    "    window_size=4,\n",
    "    max_lag=16,\n",
    "):\n",
    "    # Merge on stat_week_sunday\n",
    "    df = pd.merge(\n",
    "        orca_df[[\"stat_week_sunday\", orca_col]],\n",
    "        salmon_df[[\"stat_week_sunday\", salmon_col]],\n",
    "        on=\"stat_week_sunday\",\n",
    "        how=\"inner\",\n",
    "    ).dropna()\n",
    "\n",
    "    # Run your preprocessing & lagged correlation function\n",
    "    lagged_df = preprocess_and_plot(\n",
    "        df,\n",
    "        orca_col=orca_col,\n",
    "        salmon_col=salmon_col,\n",
    "        window_size=window_size,\n",
    "        max_lag=max_lag,\n",
    "    )\n",
    "\n",
    "    return lagged_df\n",
    "\n",
    "\n",
    "#                                     #\n",
    "# ----------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ec3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup Codes for RMPC Catch Data\n",
    "## Lookup Species\n",
    "lookup_species = pd.DataFrame(\n",
    "    {\n",
    "        \"species\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n",
    "        \"species_name\": [\n",
    "            \"Chinook\",\n",
    "            \"Coho\",\n",
    "            \"Steelhead\",\n",
    "            \"Sockeye\",\n",
    "            \"Chum\",\n",
    "            \"Pink\",\n",
    "            \"Masu\",\n",
    "            \"Cutthroat\",\n",
    "            \"Atlantic\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "## Lookup State Code\n",
    "lookup_state_code = pd.DataFrame({\"state_code\": [\"3\"], \"state\": [\"Washington\"]})\n",
    "\n",
    "## Lookup Water Type\n",
    "lookup_water_type = pd.DataFrame(\n",
    "    {\"water_type_code\": [\"M\", \"F\"], \"water_type\": [\"Marine\", \"Freshwater\"]}\n",
    ")\n",
    "\n",
    "## Lookup Sector Type\n",
    "lookup_sector_type = pd.DataFrame(\n",
    "    {\n",
    "        \"sector_code\": [\"1\", \"2\", \"3\", \"4\", \"*\", \"5\"],\n",
    "        \"sector\": [\n",
    "            \"Puget Sound\",\n",
    "            \"Coastal Streams and Estuaries\",\n",
    "            \"Ocean\",\n",
    "            \"Columbia River and Tributaries\",\n",
    "            \"Outside Washington\",\n",
    "            \"Lakes\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "## Lookup Region\n",
    "lookup_region = pd.DataFrame(\n",
    "    {\n",
    "        \"region_code\": [\n",
    "            \"01\",\n",
    "            \"02\",\n",
    "            \"03\",\n",
    "            \"04\",\n",
    "            \"05\",\n",
    "            \"06\",\n",
    "            \"07\",\n",
    "            \"08\",\n",
    "            \"11\",\n",
    "            \"12\",\n",
    "            \"13\",\n",
    "            \"14\",\n",
    "            \"15\",\n",
    "            \"17\",\n",
    "            \"18\",\n",
    "            \"19\",\n",
    "            \"20\",\n",
    "            \"21\",\n",
    "            \"22\",\n",
    "            \"23\",\n",
    "            \"24\",\n",
    "            \"25\",\n",
    "            \"26\",\n",
    "        ],\n",
    "        \"region\": [\n",
    "            \"Nooksack / Samish Terminal\",\n",
    "            \"Skagit Terminal\",\n",
    "            \"Stillaguamish / Snohomish Terminal\",\n",
    "            \"Hood Canal Terminal\",\n",
    "            \"South Puget Sound Terminal\",\n",
    "            \"Domestic Mixed Stock\",\n",
    "            \"International Mixed Stock\",\n",
    "            \"Strait of Juan De Fuca Terminal\",\n",
    "            \"International Mixed Stock (5, 6, 7)\",\n",
    "            \"Skagit, Stillaguamish, Snohomish Terminal\",\n",
    "            \"Domestic Mixed Stock\",\n",
    "            \"South Puget Sound Terminal\",\n",
    "            \"Hood Canal Terminal\",\n",
    "            \"North Coast Streams\",\n",
    "            \"Grays Harbor Estuary\",\n",
    "            \"Willapa Harbor Estuary\",\n",
    "            \"Columbia River\",\n",
    "            \"Marine Area 1\",\n",
    "            \"Marine Area 2\",\n",
    "            \"Marine Area 3\",\n",
    "            \"Marine Area 4\",\n",
    "            \"Marine Area 5\",\n",
    "            \"Marine Area 6\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "## Location Code Lookup\n",
    "psc_stat_area_lookup = {\n",
    "    # Columbia River Zone\n",
    "    \"0A\": {\"marine_area\": \"Columbia River\", \"region\": \"Lower Columbia\", \"notes\": \"\"},\n",
    "    \"0B\": {\"marine_area\": \"Columbia River\", \"region\": \"Lower Columbia\", \"notes\": \"\"},\n",
    "    \"0C\": {\"marine_area\": \"Columbia River\", \"region\": \"Lower Columbia\", \"notes\": \"\"},\n",
    "    \"0D\": {\"marine_area\": \"Columbia River\", \"region\": \"Lower Columbia\", \"notes\": \"\"},\n",
    "    \"0E\": {\"marine_area\": \"Columbia River\", \"region\": \"Lower Columbia\", \"notes\": \"\"},\n",
    "    \"0F\": {\"marine_area\": \"Columbia River\", \"region\": \"Lower Columbia\", \"notes\": \"\"},\n",
    "    \"0G\": {\"marine_area\": \"Columbia River\", \"region\": \"Lower Columbia\", \"notes\": \"\"},\n",
    "    \"0R\": {\n",
    "        \"marine_area\": \"Columbia River (Research)\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"Research/restricted\",\n",
    "    },\n",
    "    \"0X\": {\n",
    "        \"marine_area\": \"Columbia River (Closed)\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    # Coastal Westport / Grays Harbor\n",
    "    \"1\": {\"marine_area\": \"Grays Harbor\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"1A\": {\"marine_area\": \"Grays Harbor\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"1R\": {\n",
    "        \"marine_area\": \"Grays Harbor (Research)\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Research/restricted\",\n",
    "    },\n",
    "    \"2\": {\"marine_area\": \"Westport\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"2A\": {\"marine_area\": \"Westport\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"2B\": {\"marine_area\": \"Westport\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"2C\": {\"marine_area\": \"Westport\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"2D\": {\"marine_area\": \"Westport\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"2H\": {\n",
    "        \"marine_area\": \"Westport Coastal (Special/Restricted)\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Research or tribal subarea\",\n",
    "    },\n",
    "    \"2X\": {\n",
    "        \"marine_area\": \"Westport\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    # La Push / Outer Coast\n",
    "    \"3\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3A\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3B\": {\"marine_area\": \"Quillayute / Queets\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3C\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3D\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3E\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3F\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3G\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3H\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3I\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3J\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3K\": {\"marine_area\": \"La Push\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"3X\": {\n",
    "        \"marine_area\": \"La Push\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    # Neah Bay coastal\n",
    "    \"4\": {\"marine_area\": \"Neah Bay\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"4A\": {\"marine_area\": \"Neah Bay Offshore\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"4B\": {\"marine_area\": \"Neah Bay Inshore\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"4R\": {\n",
    "        \"marine_area\": \"Neah Bay (Research)\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Research/restricted\",\n",
    "    },\n",
    "    \"4X\": {\n",
    "        \"marine_area\": \"Neah Bay (Closed)\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    # Sekiu / coastal north\n",
    "    \"5\": {\"marine_area\": \"Sekiu / Pillar Point\", \"region\": \"Coastal Zone\", \"notes\": \"\"},\n",
    "    \"5R\": {\n",
    "        \"marine_area\": \"Sekiu / Pillar Point (Research)\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Research/restricted\",\n",
    "    },\n",
    "    \"5X\": {\n",
    "        \"marine_area\": \"Sekiu / Pillar Point (Closed)\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    # Strait of Juan de Fuca / Puget Sound transition\n",
    "    \"6\": {\n",
    "        \"marine_area\": \"Strait of Juan de Fuca\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    \"6A\": {\n",
    "        \"marine_area\": \"Ediz Hook to Dungeness Spit\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    \"6B\": {\"marine_area\": \"Port Angeles\", \"region\": \"Puget Sound\", \"notes\": \"\"},\n",
    "    \"6C\": {\"marine_area\": \"Port Townsend\", \"region\": \"Puget Sound\", \"notes\": \"\"},\n",
    "    \"6D\": {\n",
    "        \"marine_area\": \"East Strait of Juan de Fuca\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    \"6R\": {\n",
    "        \"marine_area\": \"Strait of Juan de Fuca Research Zone\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"Research or restricted\",\n",
    "    },\n",
    "    \"6X\": {\n",
    "        \"marine_area\": \"Strait of Juan de Fuca (Closed)\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    # San Juan Islands / Northern Puget Sound\n",
    "    \"7\": {\"marine_area\": \"San Juan Islands\", \"region\": \"Puget Sound\", \"notes\": \"\"},\n",
    "    \"7A\": {\n",
    "        \"marine_area\": \"San Juan Islands - General\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    \"7B\": {\"marine_area\": \"Bellingham Bay\", \"region\": \"Puget Sound\", \"notes\": \"\"},\n",
    "    \"7C\": {\"marine_area\": \"Lummi Bay\", \"region\": \"Puget Sound\", \"notes\": \"\"},\n",
    "    \"7D\": {\"marine_area\": \"Drayton Harbor\", \"region\": \"Puget Sound\", \"notes\": \"\"},\n",
    "    \"7E\": {\n",
    "        \"marine_area\": \"San Juan Islands (Special/Restricted)\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"Special management subzone\",\n",
    "    },\n",
    "    \"7R\": {\n",
    "        \"marine_area\": \"San Juan Islands (Research)\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"Research or restricted\",\n",
    "    },\n",
    "    \"7X\": {\n",
    "        \"marine_area\": \"San Juan Islands (Closed)\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    # Possession Sound / Everett\n",
    "    \"8\": {\n",
    "        \"marine_area\": \"Possession Sound / Everett\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    \"8A\": {\n",
    "        \"marine_area\": \"Saratoga Passage / Skagit Bay\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    \"8D\": {\n",
    "        \"marine_area\": \"Possession Sound / Everett\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"\",\n",
    "    },\n",
    "    \"8R\": {\n",
    "        \"marine_area\": \"Possession Sound Research Zone\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"Research or restricted\",\n",
    "    },\n",
    "    # Admiralty Inlet\n",
    "    \"9\": {\"marine_area\": \"Admiralty Inlet\", \"region\": \"Puget Sound\", \"notes\": \"\"},\n",
    "    \"9A\": {\"marine_area\": \"Admiralty Inlet\", \"region\": \"Puget Sound\", \"notes\": \"\"},\n",
    "    # Fallback / undefined\n",
    "    \"0Z\": {\n",
    "        \"marine_area\": \"Undefined Zone\",\n",
    "        \"region\": \"Unknown\",\n",
    "        \"notes\": \"Special or undefined subarea\",\n",
    "    },\n",
    "    \"8X\": {\n",
    "        \"marine_area\": \"Possession Sound / Skagit Bay\",\n",
    "        \"region\": \"Puget Sound\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    \"1X\": {\n",
    "        \"marine_area\": \"Grays Harbor\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    \"0X\": {\n",
    "        \"marine_area\": \"Columbia River\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"Closed or experimental\",\n",
    "    },\n",
    "    \"13B\": {\n",
    "        \"marine_area\": \"La Push - Subarea B\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area, verify exact boundaries\",\n",
    "    },\n",
    "    \"10A\": {\n",
    "        \"marine_area\": \"Columbia River - Subarea A\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"Subzone of Columbia River area\",\n",
    "    },\n",
    "    \"12C\": {\n",
    "        \"marine_area\": \"Grays Harbor - Subarea C\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Grays Harbor area\",\n",
    "    },\n",
    "    \"12A\": {\n",
    "        \"marine_area\": \"Grays Harbor - Subarea A\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Grays Harbor area\",\n",
    "    },\n",
    "    \"11\": {\n",
    "        \"marine_area\": \"Coastal Zone - Area 11\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"General coastal zone area 11\",\n",
    "    },\n",
    "    \"13A\": {\n",
    "        \"marine_area\": \"La Push - Subarea A\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"12\": {\n",
    "        \"marine_area\": \"Grays Harbor - Area 12\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"General Grays Harbor area\",\n",
    "    },\n",
    "    \"12B\": {\n",
    "        \"marine_area\": \"Grays Harbor - Subarea B\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Grays Harbor area\",\n",
    "    },\n",
    "    \"12D\": {\n",
    "        \"marine_area\": \"Grays Harbor - Subarea D\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Grays Harbor area\",\n",
    "    },\n",
    "    \"13\": {\n",
    "        \"marine_area\": \"La Push - Area 13\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"General La Push area\",\n",
    "    },\n",
    "    \"10\": {\n",
    "        \"marine_area\": \"Columbia River - Area 10\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"General Columbia River area\",\n",
    "    },\n",
    "    \"11A\": {\n",
    "        \"marine_area\": \"Coastal Zone - Subarea A\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of area 11\",\n",
    "    },\n",
    "    \"10E\": {\n",
    "        \"marine_area\": \"Columbia River - Subarea E\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"Subzone of Columbia River area\",\n",
    "    },\n",
    "    \"10Z\": {\n",
    "        \"marine_area\": \"Columbia River - Special Zone Z\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"Special or undefined subarea\",\n",
    "    },\n",
    "    \"10B\": {\n",
    "        \"marine_area\": \"Columbia River - Subarea B\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"Subzone of Columbia River area\",\n",
    "    },\n",
    "    \"13C\": {\n",
    "        \"marine_area\": \"La Push - Subarea C\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"13D\": {\n",
    "        \"marine_area\": \"La Push - Subarea D\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"13G\": {\n",
    "        \"marine_area\": \"La Push - Subarea G\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"13E\": {\n",
    "        \"marine_area\": \"La Push - Subarea E\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"13F\": {\n",
    "        \"marine_area\": \"La Push - Subarea F\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"13H\": {\n",
    "        \"marine_area\": \"La Push - Subarea H\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"13I\": {\n",
    "        \"marine_area\": \"La Push - Subarea I\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"13K\": {\n",
    "        \"marine_area\": \"La Push - Subarea K\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"13J\": {\n",
    "        \"marine_area\": \"La Push - Subarea J\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of La Push area\",\n",
    "    },\n",
    "    \"2M\": {\n",
    "        \"marine_area\": \"Westport - Subarea M\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Westport area\",\n",
    "    },\n",
    "    \"2G\": {\n",
    "        \"marine_area\": \"Westport - Subarea G\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Westport area\",\n",
    "    },\n",
    "    \"2J\": {\n",
    "        \"marine_area\": \"Westport - Subarea J\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Westport area\",\n",
    "    },\n",
    "    \"2K\": {\n",
    "        \"marine_area\": \"Westport - Subarea K\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Westport area\",\n",
    "    },\n",
    "    \"0\": {\n",
    "        \"marine_area\": \"Columbia River - Area 0\",\n",
    "        \"region\": \"Lower Columbia\",\n",
    "        \"notes\": \"General Columbia River area\",\n",
    "    },\n",
    "    \"54\": {\n",
    "        \"marine_area\": \"Unknown Area 54\",\n",
    "        \"region\": \"Unknown\",\n",
    "        \"notes\": \"Placeholder - verify info\",\n",
    "    },\n",
    "    \"1B\": {\n",
    "        \"marine_area\": \"Grays Harbor - Subarea B\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Grays Harbor area\",\n",
    "    },\n",
    "    \"12H\": {\n",
    "        \"marine_area\": \"Grays Harbor - Subarea H\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Grays Harbor area\",\n",
    "    },\n",
    "    \"2N\": {\n",
    "        \"marine_area\": \"Westport - Subarea N\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Westport area\",\n",
    "    },\n",
    "    \"2R\": {\n",
    "        \"marine_area\": \"Westport - Subarea R\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Westport area\",\n",
    "    },\n",
    "    \"2T\": {\n",
    "        \"marine_area\": \"Westport - Subarea T\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Westport area\",\n",
    "    },\n",
    "    \"2U\": {\n",
    "        \"marine_area\": \"Westport - Subarea U\",\n",
    "        \"region\": \"Coastal Zone\",\n",
    "        \"notes\": \"Subzone of Westport area\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54128748",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Get WDFW Catch Data from RMPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff3661",
   "metadata": {},
   "source": [
    "### Query Marine Zone GeoLayer from WDFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519504e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Marine Areas from WDFW\n",
    "\n",
    "marine_area_path_folder = (\n",
    "    \"/Users/tylerstevenson/Documents/CODE/SalmonSignal/data/processed/GIS/marine_zones\"\n",
    ")\n",
    "url = \"https://geodataservices.wdfw.wa.gov/arcgis/rest/services/ApplicationServices/Marine_Areas/MapServer/3/query\"\n",
    "\n",
    "# QUERY - Query for Marine Area Boundary Polygons\n",
    "params = {\n",
    "    \"where\": \"1=1\",\n",
    "    \"outFields\": \"maNumber\",\n",
    "    \"returnDistinctValues\": \"true\",\n",
    "    \"returnGeometry\": \"false\",  # ‚ú® fix\n",
    "    \"f\": \"json\",\n",
    "}\n",
    "\n",
    "r = requests.get(url, params=params)\n",
    "r.raise_for_status()\n",
    "\n",
    "features = r.json()[\"features\"]\n",
    "unique_numbers = [f[\"attributes\"][\"maNumber\"] for f in features]\n",
    "\n",
    "marine_area_gdf = []\n",
    "for ma_number in unique_numbers:\n",
    "    params = {\n",
    "        \"where\": f\"maNumber = '{ma_number}'\",\n",
    "        \"outFields\": \"*\",\n",
    "        \"returnGeometry\": \"true\",\n",
    "        \"f\": \"geojson\",\n",
    "    }\n",
    "\n",
    "    # 2. Fetch the GeoJSON\n",
    "    r = requests.get(url, params=params)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # 3. Read into GeoPandas\n",
    "    marine_area = gpd.read_file(BytesIO(r.content))\n",
    "    marine_area_gdf.append(marine_area)\n",
    "\n",
    "# Combine\n",
    "marine_area_gdf = pd.concat(marine_area_gdf)\n",
    "\n",
    "# Organize for Output\n",
    "marine_area_gdf = marine_area_gdf.rename(\n",
    "    columns={\"maName\": \"NAME\", \"maNumber\": \"MARINE_AREA\"}\n",
    ")\n",
    "marine_area_gdf = marine_area_gdf[[\"WAC\", \"MARINE_AREA\", \"NAME\", \"geometry\"]]\n",
    "marine_area_gdf = marine_area_gdf.reset_index(drop=True)\n",
    "marine_area_gdf[\"MARINE_AREA\"] = marine_area_gdf[\"MARINE_AREA\"].astype(str)\n",
    "marine_area_gdf = marine_area_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "if not os.path.exists(marine_area_path_folder):\n",
    "    os.makedirs(marine_area_path_folder)\n",
    "\n",
    "marine_area_gdf.to_parquet(f\"{marine_area_path_folder}/WDFW_MARINE_AREAS_LARGE.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9bccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open WDFW Marine Areas Polygons\n",
    "wdfw_marine_areas = gpd.read_parquet(\n",
    "    f\"{marine_area_path_folder}/WDFW_MARINE_AREAS_LARGE.parquet\"\n",
    ")\n",
    "wdfw_marine_areas = wdfw_marine_areas.rename(\n",
    "    columns={\"MARINE_AREA\": \"MARINE_AREA_LARGE\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c62b9",
   "metadata": {},
   "source": [
    "### Query RMPC Data - WDFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location Lookup\n",
    "lc_data_path = download_rmpc_data()\n",
    "\n",
    "# RMPC Catch Data\n",
    "rmpc_catch_data = update_wdfw_parquet(\n",
    "    output_dir=\"../data/raw/RMPC/WDFW\",\n",
    "    base_url=\"https://www.rmpc.org/pub/data/\",\n",
    "    pattern=\"CS042_WDFW_.*\\\\.csv\",\n",
    ")\n",
    "\n",
    "# RMPC Recovery Data\n",
    "rmpc_recovery_data = update_wdfw_parquet(\n",
    "    output_dir=\"../data/raw/RMPC/WDFW\",\n",
    "    base_url=\"https://www.rmpc.org/pub/data/\",\n",
    "    pattern=\"RC042_WDFW_.*\\\\.csv\",\n",
    ")\n",
    "\n",
    "# RMPC Release Data\n",
    "rmpc_release_data = update_wdfw_parquet(\n",
    "    output_dir=\"../data/raw/RMPC/WDFW\",\n",
    "    base_url=\"https://www.rmpc.org/pub/data/\",\n",
    "    pattern=\"RL042_WDFW_.*\\\\.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess RMPC Data\n",
    "## Location Lookup\n",
    "lc = pd.read_csv(lc_data_path)\n",
    "lc.columns = [str.strip(i) for i in lc.columns]\n",
    "\n",
    "## Catch Data\n",
    "cs = pd.read_parquet(rmpc_catch_data)\n",
    "cs = preprocess_rmpc_catch_data(cs)\n",
    "\n",
    "## Recovery Data\n",
    "rc = pd.read_parquet(rmpc_recovery_data)\n",
    "rc = preprocess_rmpc_recovery_data(rc)\n",
    "\n",
    "## Release Data\n",
    "rl = pd.read_parquet(rmpc_release_data)\n",
    "# rl = preprocess_rmpc_release_data(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Processing - Collect Areas\n",
    "cs_processed = main_preprocessing_catch(\n",
    "    cs, lookup_state_code, lookup_water_type, lookup_sector_type, psc_stat_area_lookup\n",
    ")\n",
    "\n",
    "# Main Processing - Map to Collected Areas\n",
    "cs_mapped = pd.merge(cs, cs_processed, how=\"left\")\n",
    "cs_mapped[\"MARINE_AREA_LARGE\"] = cs_mapped[\"stat_area_name\"].apply(extract_general_code)\n",
    "\n",
    "#################################\n",
    "\n",
    "# Main Processing - Collect Areas\n",
    "rc_processed = main_preprocessing_recover(\n",
    "    rc, lookup_state_code, lookup_water_type, lookup_sector_type, psc_stat_area_lookup\n",
    ")\n",
    "\n",
    "# Main Processing - Map to Collected Areas\n",
    "rc_mapped = pd.merge(rc, rc_processed, how=\"left\")\n",
    "rc_mapped[\"MARINE_AREA_LARGE\"] = rc_mapped[\"stat_area_name\"].apply(extract_general_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Large-Area Sums\n",
    "\n",
    "#################################\n",
    "## CATCH\n",
    "## CS Data\n",
    "cs_mapped_sums = cs_mapped[\n",
    "    [\"stat_week_sunday\", \"MARINE_AREA_LARGE\", \"number_caught\", \"species\"]\n",
    "]\n",
    "cs_mapped_sums = cs_mapped_sums.groupby(\n",
    "    [\"MARINE_AREA_LARGE\", \"stat_week_sunday\", \"species\"], as_index=False\n",
    ")[\"number_caught\"].sum()\n",
    "cs_mapped_sums[\"MARINE_AREA_LARGE\"] = cs_mapped_sums[\"MARINE_AREA_LARGE\"].astype(str)\n",
    "\n",
    "# Add Geometry\n",
    "cs_mapped_sums = pd.merge(cs_mapped_sums, wdfw_marine_areas)\n",
    "\n",
    "# Add Species Name\n",
    "cs_mapped_sums = pd.merge(cs_mapped_sums, lookup_species)\n",
    "\n",
    "# Fill Out - Catch\n",
    "cs_mapped_sums_filled = get_time_series_filled_out(cs_mapped_sums)\n",
    "\n",
    "#################################\n",
    "## RECOVERY\n",
    "## RC Data\n",
    "rc_mapped_sums = rc_mapped[\n",
    "    [\"stat_week_sunday\", \"MARINE_AREA_LARGE\", \"number_caught\", \"species\"]\n",
    "]\n",
    "rc_mapped_sums = rc_mapped_sums.groupby(\n",
    "    [\"MARINE_AREA_LARGE\", \"stat_week_sunday\", \"species\"], as_index=False\n",
    ")[\"number_caught\"].sum()\n",
    "rc_mapped_sums[\"MARINE_AREA_LARGE\"] = rc_mapped_sums[\"MARINE_AREA_LARGE\"].astype(str)\n",
    "\n",
    "# Add Geometry\n",
    "rc_mapped_sums = pd.merge(rc_mapped_sums, wdfw_marine_areas)\n",
    "\n",
    "# Add Species Name\n",
    "rc_mapped_sums = pd.merge(rc_mapped_sums, lookup_species)\n",
    "\n",
    "# Fill Out - Recovery\n",
    "rc_mapped_sums_filled = get_time_series_filled_out(rc_mapped_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_mapped_sums_filled[\"MARINE_AREA_LARGE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0210d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_mapped_sums_filled[\n",
    "    cs_mapped_sums_filled[\"MARINE_AREA_LARGE\"].isin([\"1\", \"2\"])\n",
    "].to_parquet(\n",
    "    \"/Users/tylerstevenson/Documents/CODE/SalmonSignal/data/raw/RMPC/WDFW/processed/MA12_RMPC_CATCH.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "108987ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_mapped_sums_filled_ = cs_mapped_sums_filled[\n",
    "    cs_mapped_sums_filled[\"MARINE_AREA_LARGE\"].isin([\"1\", \"2\"])\n",
    "][['MARINE_AREA_LARGE', 'stat_week_sunday', 'species_name', 'number_caught']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb402cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_mapped_sums_filled_.to_parquet(\"/Users/tylerstevenson/Documents/CODE/SalmonSignal/data/raw/RMPC/WDFW/processed/MA12_RMPC_CATCH.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd61668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2648c056",
   "metadata": {},
   "source": [
    "### Investigation Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Chinook\n",
    "## Chinook Catch\n",
    "cs_chinook = cs_mapped_sums_filled[cs_mapped_sums_filled.species_name == \"Chinook\"]\n",
    "\n",
    "## Chinook Recovery\n",
    "rc_chinook = rc_mapped_sums_filled[rc_mapped_sums_filled.species_name == \"Chinook\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08179110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Totals\n",
    "## Chinook Catch\n",
    "cs_chinook_total = cs_chinook.groupby([\"stat_week_sunday\"], as_index=False)[\n",
    "    \"number_caught\"\n",
    "].sum()\n",
    "\n",
    "## Chinook Recovery\n",
    "rc_chinook_total = rc_chinook.groupby([\"stat_week_sunday\"], as_index=False)[\n",
    "    \"number_caught\"\n",
    "].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919287b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(title=\"Chinook - All Marine Areas\")\n",
    "fig.add_scatter(\n",
    "    x=cs_chinook_total[\"stat_week_sunday\"],\n",
    "    y=cs_chinook_total[\"number_caught\"],\n",
    "    name=\"Catch Data\",\n",
    "    marker_color=\"#8EF9F3\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=rc_chinook_total[\"stat_week_sunday\"],\n",
    "    y=rc_chinook_total[\"number_caught\"],\n",
    "    name=\"Recovery Data\",\n",
    "    marker_color=\"#754043\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",  # White plot area\n",
    "    paper_bgcolor=\"white\",  # White outer area\n",
    "    font=dict(\n",
    "        family=\"Futura, Arial, sans-serif\",  # Or any font you love\n",
    "        size=14,\n",
    "        color=\"black\",\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgrey\",  # Optional: light gridlines\n",
    "        zeroline=False,\n",
    "    ),\n",
    "    yaxis=dict(showgrid=True, gridcolor=\"lightgrey\", zeroline=False),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca1b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "marine_area_query = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_chinook_ma_query = cs_chinook[\n",
    "    cs_chinook.MARINE_AREA_LARGE.str.contains(marine_area_query)\n",
    "]\n",
    "cs_chinook_ma_query = cs_chinook_ma_query.groupby([\"stat_week_sunday\"], as_index=False)[\n",
    "    \"number_caught\"\n",
    "].sum()\n",
    "\n",
    "rc_chinook_ma_query = rc_chinook[\n",
    "    rc_chinook.MARINE_AREA_LARGE.str.contains(marine_area_query)\n",
    "]\n",
    "rc_chinook_ma_query = rc_chinook_ma_query.groupby([\"stat_week_sunday\"], as_index=False)[\n",
    "    \"number_caught\"\n",
    "].sum()\n",
    "\n",
    "\n",
    "fig = px.line(title=f\"Chinook - Marine Area {marine_area_query}\")\n",
    "fig.add_scatter(\n",
    "    x=cs_chinook_ma_query[\"stat_week_sunday\"],\n",
    "    y=cs_chinook_ma_query[\"number_caught\"],\n",
    "    name=\"Catch Data\",\n",
    "    marker_color=\"#8EF9F3\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=rc_chinook_ma_query[\"stat_week_sunday\"],\n",
    "    y=rc_chinook_ma_query[\"number_caught\"],\n",
    "    name=\"Recovery Data\",\n",
    "    marker_color=\"#754043\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",  # White plot area\n",
    "    paper_bgcolor=\"white\",  # White outer area\n",
    "    font=dict(\n",
    "        family=\"Futura, Arial, sans-serif\",  # Or any font you love\n",
    "        size=14,\n",
    "        color=\"black\",\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgrey\",  # Optional: light gridlines\n",
    "        zeroline=False,\n",
    "    ),\n",
    "    yaxis=dict(showgrid=True, gridcolor=\"lightgrey\", zeroline=False),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_chinook_ma_query[\"STANDARIZED_CAUGHT\"] = (\n",
    "    cs_chinook_ma_query[\"number_caught\"] - cs_chinook_ma_query[\"number_caught\"].mean()\n",
    ") / cs_chinook_ma_query[\"number_caught\"].std()\n",
    "\n",
    "#################################################\n",
    "\n",
    "rc_chinook_ma_query[\"STANDARIZED_CAUGHT\"] = (\n",
    "    rc_chinook_ma_query[\"number_caught\"] - rc_chinook_ma_query[\"number_caught\"].mean()\n",
    ") / rc_chinook_ma_query[\"number_caught\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(title=\"Chinook - Marine Area 8 (Standardized)\")\n",
    "fig.add_scatter(\n",
    "    x=cs_chinook_ma_query[\"stat_week_sunday\"],\n",
    "    y=cs_chinook_ma_query[\"STANDARIZED_CAUGHT\"],\n",
    "    name=\"Catch Data\",\n",
    "    marker_color=\"#8EF9F3\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=rc_chinook_ma_query[\"stat_week_sunday\"],\n",
    "    y=rc_chinook_ma_query[\"STANDARIZED_CAUGHT\"],\n",
    "    name=\"Recovery Data\",\n",
    "    marker_color=\"#754043\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",  # White plot area\n",
    "    paper_bgcolor=\"white\",  # White outer area\n",
    "    font=dict(\n",
    "        family=\"Futura, Arial, sans-serif\",  # Or any font you love\n",
    "        size=14,\n",
    "        color=\"black\",\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgrey\",  # Optional: light gridlines\n",
    "        zeroline=False,\n",
    "    ),\n",
    "    yaxis=dict(showgrid=True, gridcolor=\"lightgrey\", zeroline=False),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882ed24",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Compare Against Orca Sightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set H3 Resolution\n",
    "h3_resolution = 3\n",
    "\n",
    "## TMW Data Path\n",
    "tmw_directory = \"/Users/tylerstevenson/Documents/CODE/orcasalmon/data/twm\"\n",
    "\n",
    "## Acartia Data Path\n",
    "acartia_directory = \"/Users/tylerstevenson/Documents/CODE/FindMyWhale/data/raw/sightings/acartia-export.csv\"\n",
    "\n",
    "## Open TMW\n",
    "tmw_data_cleaned = load_and_process_sighting_data(\n",
    "    directory=tmw_directory,\n",
    "    date_col=\"SIGHTDATE\",\n",
    "    lat_col=\"LATITUDE\",\n",
    "    lon_col=\"LONGITUDE\",\n",
    "    id_col=\"DATE\",  # or other proxy for sightings count\n",
    "    h3_resolution=h3_resolution,\n",
    "    source=\"TMW\",\n",
    ")\n",
    "\n",
    "## Open Acartia\n",
    "acartia_data_cleaned = load_and_process_sighting_data(\n",
    "    directory=acartia_directory,\n",
    "    date_col=\"CREATED\",\n",
    "    lat_col=\"LATITUDE\",\n",
    "    lon_col=\"LONGITUDE\",\n",
    "    id_col=\"ENTRY_ID\",\n",
    "    h3_resolution=h3_resolution,\n",
    "    start_date=\"2022-01-01\",\n",
    "    source=\"ACARTIA\",\n",
    ")\n",
    "\n",
    "# Conbine Sightings Data\n",
    "sightings_data_raw = pd.concat([acartia_data_cleaned, tmw_data_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef824de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sightings_gdf = gpd.GeoDataFrame(\n",
    "    sightings_data_raw,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        sightings_data_raw.LONGITUDE, sightings_data_raw.LATITUDE\n",
    "    ),\n",
    "    crs=\"EPSG:4326\",  # WGS84\n",
    ")\n",
    "\n",
    "# Ensure datetime\n",
    "sightings_gdf[\"DATE\"] = pd.to_datetime(sightings_gdf[\"DATE\"])\n",
    "\n",
    "# Join with marine areas\n",
    "wdfw_marine_areas = wdfw_marine_areas.to_crs(\"EPSG:4326\")\n",
    "orca_gdf = sightings_gdf.sjoin(wdfw_marine_areas)\n",
    "\n",
    "# Aggregate by Marine Area + Date\n",
    "orca_gdf = orca_gdf.groupby([\"DATE\", \"MARINE_AREA_LARGE\"], as_index=False)[\n",
    "    \"SIGHTING_COUNT\"\n",
    "].sum()\n",
    "\n",
    "# Calculate the Sunday of each week\n",
    "orca_gdf[\"stat_week_sunday\"] = (\n",
    "    orca_gdf[\"DATE\"]\n",
    "    - pd.to_timedelta(orca_gdf[\"DATE\"].dt.weekday, unit=\"d\")\n",
    "    - pd.Timedelta(days=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_gdf = orca_gdf.groupby([\"stat_week_sunday\", \"MARINE_AREA_LARGE\"], as_index=False)[\n",
    "    \"SIGHTING_COUNT\"\n",
    "].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Total sightings per marine area per week\n",
    "region_week_counts = (\n",
    "    orca_gdf.groupby([\"stat_week_sunday\", \"MARINE_AREA_LARGE\"])[\"SIGHTING_COUNT\"]\n",
    "    .sum()\n",
    "    .rename(\"SIGHTINGS_COUNT\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 3: Total sightings per week across all marine areas\n",
    "weekly_totals = (\n",
    "    region_week_counts.groupby(\"stat_week_sunday\")[\"SIGHTINGS_COUNT\"]\n",
    "    .sum()\n",
    "    .rename(\"TOTAL_WEEKLY_SIGHTINGS\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 4: Merge + calculate ratio\n",
    "orca_share = region_week_counts.merge(weekly_totals, on=\"stat_week_sunday\", how=\"left\")\n",
    "orca_share[\"SIGHTINGS_RATIO\"] = (\n",
    "    orca_share[\"SIGHTINGS_COUNT\"] / orca_share[\"TOTAL_WEEKLY_SIGHTINGS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26598283",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_area_8 = orca_share[(orca_share.MARINE_AREA_LARGE.str.contains(marine_area_query))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_area_8 = orca_area_8.groupby([\"stat_week_sunday\"], as_index=False).agg(\n",
    "    SIGHTINGS_COUNT=(\"SIGHTINGS_COUNT\", \"sum\"),\n",
    "    TOTAL_WEEKLY_SIGHTINGS=(\"TOTAL_WEEKLY_SIGHTINGS\", \"sum\"),\n",
    ")\n",
    "orca_area_8[\"SIGHTINGS_RATIO\"] = (\n",
    "    orca_area_8[\"SIGHTINGS_COUNT\"] / orca_area_8[\"TOTAL_WEEKLY_SIGHTINGS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Base figure from px just to set up layout\n",
    "fig = px.line(title=f\"Chinook - Marine Area {marine_area_query}\", height=500)\n",
    "\n",
    "# Catch Data (Primary Y)\n",
    "fig.add_scatter(\n",
    "    x=cs_chinook_ma_query[\"stat_week_sunday\"],\n",
    "    y=cs_chinook_ma_query[\"STANDARIZED_CAUGHT\"],\n",
    "    name=\"Catch Data\",\n",
    "    marker_color=\"#8EF9F3\",\n",
    "    yaxis=\"y1\",\n",
    ")\n",
    "\n",
    "# Recovery Data (Primary Y)\n",
    "fig.add_scatter(\n",
    "    x=rc_chinook_ma_query[\"stat_week_sunday\"],\n",
    "    y=rc_chinook_ma_query[\"STANDARIZED_CAUGHT\"],\n",
    "    name=\"Recovery Data\",\n",
    "    marker_color=\"#754043\",\n",
    "    yaxis=\"y1\",\n",
    ")\n",
    "\n",
    "# Orca Sightings Ratio (Secondary Y)\n",
    "fig.add_scatter(\n",
    "    x=orca_area_8[\"stat_week_sunday\"],\n",
    "    y=orca_area_8[\"SIGHTINGS_COUNT\"],\n",
    "    name=\"Orca Sightings Ratio of Total\",\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"#EF476F\", dash=\"dot\"),\n",
    "    yaxis=\"y2\",\n",
    ")\n",
    "\n",
    "# Update layout for dual Y-axes\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    font=dict(\n",
    "        family=\"Futura, Arial, sans-serif\",\n",
    "        size=14,\n",
    "        color=\"black\",\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Week\",\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgrey\",\n",
    "        zeroline=False,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Chinook (Standardized)\",\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgrey\",\n",
    "        zeroline=False,\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Orca Sightings Ratio Total\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c865a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Step 1: Monthly aggregation\n",
    "cs_monthly = cs_chinook_ma_query.copy()\n",
    "cs_monthly[\"month\"] = cs_monthly[\"stat_week_sunday\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "cs_monthly = cs_monthly.groupby(\"month\")[\"STANDARIZED_CAUGHT\"].mean().reset_index()\n",
    "\n",
    "rc_monthly = rc_chinook_ma_query.copy()\n",
    "rc_monthly[\"month\"] = rc_monthly[\"stat_week_sunday\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "rc_monthly = rc_monthly.groupby(\"month\")[\"STANDARIZED_CAUGHT\"].mean().reset_index()\n",
    "\n",
    "orca_monthly = orca_area_8.copy()\n",
    "orca_monthly[\"month\"] = (\n",
    "    orca_monthly[\"stat_week_sunday\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    ")\n",
    "orca_monthly = orca_monthly.groupby(\"month\")[\"SIGHTINGS_COUNT\"].mean().reset_index()\n",
    "\n",
    "# Step 2: Base figure\n",
    "fig = px.line(title=f\"Chinook - Marine Area {marine_area_query} (Monthly)\", height=500)\n",
    "\n",
    "# Step 3: Add Traces\n",
    "# Catch Data\n",
    "fig.add_scatter(\n",
    "    x=cs_monthly[\"month\"],\n",
    "    y=cs_monthly[\"STANDARIZED_CAUGHT\"],\n",
    "    name=\"Catch Data\",\n",
    "    marker_color=\"#8EF9F3\",\n",
    "    yaxis=\"y1\",\n",
    ")\n",
    "\n",
    "# Recovery Data\n",
    "fig.add_scatter(\n",
    "    x=rc_monthly[\"month\"],\n",
    "    y=rc_monthly[\"STANDARIZED_CAUGHT\"],\n",
    "    name=\"Recovery Data\",\n",
    "    marker_color=\"#754043\",\n",
    "    yaxis=\"y1\",\n",
    ")\n",
    "\n",
    "# Orca Sightings Ratio\n",
    "fig.add_scatter(\n",
    "    x=orca_monthly[\"month\"],\n",
    "    y=orca_monthly[\"SIGHTINGS_COUNT\"],\n",
    "    name=\"Orca Sightings Ratio of Total\",\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"#EF476F\", dash=\"dot\"),\n",
    "    yaxis=\"y2\",\n",
    ")\n",
    "\n",
    "# Step 4: Update layout for dual y-axes\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    font=dict(\n",
    "        family=\"Futura, Arial, sans-serif\",\n",
    "        size=14,\n",
    "        color=\"black\",\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"Month\",\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgrey\",\n",
    "        zeroline=False,\n",
    "        tickformat=\"%b\\n%Y\",  # e.g., Jan 2023\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Chinook (Standardized)\",\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgrey\",\n",
    "        zeroline=False,\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Orca Sightings Ratio Total\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "        showgrid=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30184f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Orca Data ---\n",
    "orca_box = orca_area_8.copy()\n",
    "orca_box[\"month_name\"] = orca_box[\"stat_week_sunday\"].dt.month_name()\n",
    "\n",
    "# Standardize sightings ratio\n",
    "scaler = StandardScaler()\n",
    "orca_box[\"value\"] = scaler.fit_transform(orca_box[[\"SIGHTINGS_COUNT\"]])\n",
    "orca_box[\"source\"] = \"Orca Sightings\"\n",
    "\n",
    "# Salmon Catch Data\n",
    "catch_box = cs_chinook_ma_query.copy()\n",
    "catch_box[\"month_name\"] = catch_box[\"stat_week_sunday\"].dt.month_name()\n",
    "catch_box[\"value\"] = catch_box[\"STANDARIZED_CAUGHT\"]\n",
    "catch_box[\"source\"] = \"Salmon Catch\"\n",
    "\n",
    "# Salmon Release Data\n",
    "release_box = rc_chinook_ma_query.copy()\n",
    "release_box[\"month_name\"] = release_box[\"stat_week_sunday\"].dt.month_name()\n",
    "release_box[\"value\"] = release_box[\"STANDARIZED_CAUGHT\"]\n",
    "release_box[\"source\"] = \"Salmon Recovery\"  # Change if you use actual release data\n",
    "\n",
    "# Combine all into one tidy dataframe\n",
    "combined_box = pd.concat([orca_box, catch_box, release_box], axis=0)\n",
    "\n",
    "# Ensure month order\n",
    "month_order = [\n",
    "    \"January\",\n",
    "    \"February\",\n",
    "    \"March\",\n",
    "    \"April\",\n",
    "    \"May\",\n",
    "    \"June\",\n",
    "    \"July\",\n",
    "    \"August\",\n",
    "    \"September\",\n",
    "    \"October\",\n",
    "    \"November\",\n",
    "    \"December\",\n",
    "]\n",
    "combined_box[\"month_name\"] = pd.Categorical(\n",
    "    combined_box[\"month_name\"], categories=month_order, ordered=True\n",
    ")\n",
    "\n",
    "# Plot!\n",
    "fig = px.box(\n",
    "    combined_box,\n",
    "    x=\"month_name\",\n",
    "    y=\"value\",\n",
    "    color=\"source\",\n",
    "    title=f\"Monthly Distributions of Orca Sightings, Salmon Catch & Recovery - Marine Area {marine_area_query}\",\n",
    "    labels={\"month_name\": \"Month\", \"value\": \"Standardized Value\"},\n",
    "    color_discrete_map={\n",
    "        \"Orca Sightings\": \"#EF476F\",\n",
    "        \"Salmon Catch\": \"#8EF9F3\",\n",
    "        \"Salmon Recovery\": \"#754043\",\n",
    "    },\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    paper_bgcolor=\"white\",\n",
    "    font=dict(family=\"Futura, Arial\", size=14, color=\"black\"),\n",
    "    xaxis_title=None,\n",
    "    yaxis_title=\"Standardized or Ratio Value\",\n",
    "    boxmode=\"group\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ada29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9be7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5  # 4 weeks for smoothing peaks\n",
    "\n",
    "orca_area_8[\"SIGHTINGS_RATIO_SMOOTH\"] = (\n",
    "    orca_area_8[\"SIGHTINGS_COUNT\"].rolling(window=window_size, center=True).mean()\n",
    ")\n",
    "cs_chinook_ma_query[\"STANDARIZED_CAUGHT_SMOOTH\"] = (\n",
    "    cs_chinook_ma_query[\"STANDARIZED_CAUGHT\"]\n",
    "    .rolling(window=window_size, center=True)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "df = pd.merge(\n",
    "    orca_area_8[[\"stat_week_sunday\", \"SIGHTINGS_RATIO_SMOOTH\"]],\n",
    "    cs_chinook_ma_query[[\"stat_week_sunday\", \"STANDARIZED_CAUGHT_SMOOTH\"]],\n",
    "    on=\"stat_week_sunday\",\n",
    "    how=\"inner\",\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e0d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "max_lag = 16  # weeks, adjust as you want\n",
    "results = []\n",
    "\n",
    "for lag in range(-max_lag, max_lag + 1):\n",
    "    shifted = df[\"SIGHTINGS_RATIO_SMOOTH\"].shift(lag)\n",
    "    valid = pd.concat([df[\"STANDARIZED_CAUGHT_SMOOTH\"], shifted], axis=1).dropna()\n",
    "    if len(valid) > 0:\n",
    "        r, _ = pearsonr(\n",
    "            valid[\"STANDARIZED_CAUGHT_SMOOTH\"], valid[\"SIGHTINGS_RATIO_SMOOTH\"]\n",
    "        )\n",
    "        results.append((lag, r))\n",
    "    else:\n",
    "        results.append((lag, float(\"nan\")))\n",
    "\n",
    "lags_df = pd.DataFrame(results, columns=[\"lag_weeks\", \"correlation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data: drop NaNs and align\n",
    "df_clean = df.dropna(subset=[\"SIGHTINGS_RATIO_SMOOTH\", \"STANDARIZED_CAUGHT_SMOOTH\"])\n",
    "\n",
    "X = df_clean[\"STANDARIZED_CAUGHT_SMOOTH\"].values.reshape(-1, 1)\n",
    "y = df_clean[\"SIGHTINGS_RATIO_SMOOTH\"].values\n",
    "\n",
    "mi = mutual_info_regression(X, y, random_state=42)\n",
    "print(f\"Mutual Information: {mi[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dcor\n",
    "\n",
    "# Make sure arrays aligned and drop NaNs\n",
    "x = df_clean[\"STANDARIZED_CAUGHT_SMOOTH\"].values\n",
    "y = df_clean[\"SIGHTINGS_RATIO_SMOOTH\"].values\n",
    "\n",
    "dcorr = dcor.distance_correlation(x, y)\n",
    "print(f\"Distance Correlation: {dcorr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_df = orca_area_8\n",
    "salmon_df = cs_chinook_ma_query\n",
    "\n",
    "run_full_analysis(\n",
    "    orca_df,\n",
    "    salmon_df,\n",
    "    orca_col=\"SIGHTINGS_RATIO\",\n",
    "    salmon_col=\"STANDARIZED_CAUGHT\",\n",
    "    window_size=4,\n",
    "    max_lag=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823e965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a22d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # API endpoint\n",
    "# url = \"https://data.wa.gov/resource/auvb-4rvk.json\"\n",
    "\n",
    "# # Optional: params like $limit, $where, etc.\n",
    "# params = {\"$limit\": 1000}  # Increase this or paginate if needed\n",
    "\n",
    "# # Make the request\n",
    "# response = requests.get(url, params=params)\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# if response.status_code == 200:\n",
    "#     data = response.json()\n",
    "#     df_tag_recovery = pd.DataFrame(data)\n",
    "# else:\n",
    "#     print(f\"Error: {response.status_code}\")\n",
    "\n",
    "df_tag_recovery = pd.read_csv(\n",
    "    \"/Users/tylerstevenson/Documents/CODE/SalmonSignal/data/raw/RMPC/WDFW/WDFW-Coded_Wire_Tag_Fish_Recoveries_20250730.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Data and Save to File\n",
    "columns_to_use = [\n",
    "    \"Species\",\n",
    "    \"Recovery Date\",\n",
    "    \"Location Name\",\n",
    "    \"PSC Code\",\n",
    "    \"Location Code\",\n",
    "]\n",
    "df_tag_recovery = df_tag_recovery[columns_to_use].copy()\n",
    "df_tag_recovery[\"Location Code\"] = df_tag_recovery[\"Location Code\"].fillna(\"\")\n",
    "df_tag_recovery[\"Count\"] = 1\n",
    "\n",
    "tag_recovered = df_tag_recovery.groupby(columns_to_use, as_index=False)[\"Count\"].sum()\n",
    "tag_recovered = tag_recovered[tag_recovered.Species != \"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a958e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag_recovery[\"Date\"] = pd.to_datetime(df_tag_recovery[\"Recovery Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a98222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag_recovery.Date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_sums = tag_recovered.groupby([\"Recovery Date\"], as_index=False)[\"Count\"].sum()\n",
    "tag_sums[\"Date\"] = pd.to_datetime(tag_sums[\"Recovery Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af02e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter(x = tag_sums['Date'], y = tag_sums['Count'])\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a45e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salmonid_indicators = \"/Users/tylerstevenson/Documents/CODE/SalmonSignal/data/raw/WDFW_-_Salmonid_Population_Indicators_Database__SPI__Metrics_and_Indicators.csv\"\n",
    "salmonid_indicators = pd.read_csv(salmonid_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salmonid_indicators.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(salmonid_indicators.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e552a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "salmonid_indicators.iloc[0].tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d72f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salmonsignal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
